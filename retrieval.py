import os
import chromadb
import chromadb.utils.embedding_functions as embedding_functions
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# --- Configuration ---
# These should match the values used in your ingestion script
CHROMA_PERSIST_DIRECTORY = "./my_askhertsrag_db4"  # Or the path where your DB is stored
# CHROMA_PERSIST_DIRECTORY = "./my_askhertsrag_db2"  # Or the path where your DB is stored
CHROMA_COLLECTION_NAME = "herts_info_collection" # The name of your collection
# By default, Chroma uses 'all-MiniLM-L6-v2'. If you used a different one for ingestion, specify it here.
EMBEDDING_MODEL_NAME = "all-mpnet-base-v2" 
# EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2" 
# Or, for example: "all-mpnet-base-v2" if you used that in ingestion


# Check for OpenAI API Key
import dotenv
dotenv.load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    print("Error: OPENAI_API_KEY environment variable not set.")
    print("Please set it before running the script: export OPENAI_API_KEY='your_key'")
    exit()


def format_docs(docs):
    """Helper function to format retrieved documents for the prompt."""
    return "\n\n".join(doc.page_content for doc in docs)


def query_rag_with_langchain(query_text):
    """
    Queries the RAG system using Langchain, ChromaDB, and ChatOpenAI.

    Args:
        query_text (str): The user's question.

    Returns:
        str: The answer generated by the LLM, or an error message.
    """
    try:
        # 1. Initialize ChromaDB client and embedding function
        # This embedding function should match the one used during ingestion.
        embedding_function = SentenceTransformerEmbeddings(
            model_name=EMBEDDING_MODEL_NAME
        )
        
        # Initialize a persistent client
        persistent_client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIRECTORY)

        # 2. Load the existing ChromaDB collection as a Langchain VectorStore
        # We pass the client, collection name, and the embedding function.
        # Langchain's Chroma wrapper will use this to fetch and embed the query.
        vectorstore = Chroma(
            client=persistent_client,
            collection_name=CHROMA_COLLECTION_NAME,
            embedding_function=embedding_function, # Pass the embedding function object
        )
        print(f"Successfully connected to ChromaDB collection: '{CHROMA_COLLECTION_NAME}'")
        print(f"Total documents in collection: {vectorstore._collection.count()}")


        # 3. Create a retriever from the vector store
        # This retriever will fetch relevant documents based on similarity to the query.
        # k=3 means it will retrieve the top 3 most similar documents.
        retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
        print(f"Retriever created. Will fetch top {retriever.search_kwargs['k']} documents.")

        # 4. Define the prompt template
        # This template structures how the retrieved context and the question are presented to the LLM.
        template = """You are an assistant for question-answering tasks. 
        Use the following retrieved context to answer the question. 
        If you don't know the answer, just say that you don't know. 
        

        Context:
        {context}

        Question:
        {question}

        Answer:
        """
        prompt = ChatPromptTemplate.from_template(template)

        # 5. Initialize the ChatOpenAI model
        # You can specify model_name, temperature, etc.
        llm = ChatOpenAI(model_name="gpt-4o", temperature=0.4)
        print("ChatOpenAI model initialized.")

        # 6. Create the RAG chain
        # This chain defines the sequence of operations:
        # - The 'context' is fetched by the retriever and formatted by 'format_docs'.
        # - The 'question' is passed through directly.
        # - These are fed into the 'prompt'.
        # - The formatted prompt goes to the 'llm'.
        # - The LLM's output is parsed by 'StrOutputParser'.
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        print("RAG chain created.")

        # 7. Invoke the chain with the query
        print(f"\nInvoking RAG chain with query: '{query_text}'")
        answer = rag_chain.invoke(query_text)
        
        # You can also stream the response if needed:
        # print("\nStreaming response:")
        # for chunk in rag_chain.stream(query_text):
        #     print(chunk, end="", flush=True)
        # print()

        return answer

    except FileNotFoundError:
        return (f"Error: ChromaDB directory not found at '{CHROMA_PERSIST_DIRECTORY}'. "
                "Ensure you have run the ingestion script and the path is correct.")
    except Exception as e:
        return f"An error occurred: {e}"


user_query = "What are the student visa application fees and whats the cost for lost ID"
# user_query = "How do I apply to University of Hertfordshire?"
# user_query = "Tell me about recent news from Herts."
# user_query = "steps to request for my council tax exclusion. can you elaborate"


print(f"Attempting to answer query: '{user_query}'")
final_answer = query_rag_with_langchain(user_query)

print("\n--- Generated Answer ---")
print(final_answer)

# Example of how to inspect retrieved documents (optional)
# This part is for debugging or understanding what context was used.
print("\n--- For Debugging: Retrieving documents directly ---")
try:
    embedding_function_debug = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    persistent_client_debug = chromadb.PersistentClient(path=CHROMA_PERSIST_DIRECTORY)
    vectorstore_debug = Chroma(
        client=persistent_client_debug,
        collection_name=CHROMA_COLLECTION_NAME,
        embedding_function=embedding_function_debug,
    )
    retriever_debug = vectorstore_debug.as_retriever(search_kwargs={"k": 6})
    retrieved_docs = retriever_debug.invoke(user_query) # Langchain LCEL uses invoke()
    
    if retrieved_docs:
        print(f"\nRetrieved {len(retrieved_docs)} documents for the query '{user_query}':")
        for i, doc in enumerate(retrieved_docs):
            print(f"\n--- Document {i+1} (Source: {doc.metadata.get('source_url', 'N/A')}) ---")
            print(doc.page_content)
    else:
        print("No documents were retrieved for the query.")
        
except Exception as e:
    print(f"Error during debug document retrieval: {e}")